This page presnets the results of the experiments of the model flan-t5 and compare the results with the results generated by the paper

Experiment environment:

- At this stage I am still not able to have the input in a similar format to the one described by the paper ( check if this was true)

  | Name  | Type                       | Params
-----------------------------------------------------
0 | model | T5ForConditionalGeneration | 247 M
-----------------------------------------------------
247 M     Trainable params
0         Non-trainable params
247 M     Total params
990.311   Total estimated model params size (MB)



Experiement 1: 

To interpret the results of the FLAN-T5 model in a detailed and comprehensible manner, let's delve into the metrics obtained during the supervised learning training, and compare these with the findings reported by the paper "Counterfactual Story Reasoning and Generation". 

### Model Performance Overview

### Comparative Table

| Metric               | FLAN-T5 Test Results | Paper Benchmark | Interpretation                                         |
|----------------------|----------------------|---------------------------|--------------------------------------------------------|
| avg_bleu             | 0.533                | -                         | Moderate accuracy in text generation compared to targets. |
| rouge1_avg_fmeasure  | 0.754                | -                         | Good balance between precision and recall in capturing key information. |
| rouge2_avg_fmeasure  | 0.633                | -                         | Demonstrates the model's ability to replicate more complex sequences. |
| rougeL_avg_fmeasure  | 0.740                | -                         | High effectiveness in capturing longer sequences faithfully. |
| test_loss            | 1.042                | -                         | Indicates potential for improvement in prediction accuracy. |

*Note: Paper Counterfactual Story Reasoning and Generation.*

The test metrics provide a quantitative measure of the model's ability to generate text that aligns with the provided targets. Here's a breakdown:

- **BLEU Score (avg_bleu):** 0.533. This score measures the model's accuracy in reproducing the exact sequences of words in the target texts. A score closer to 1 indicates higher accuracy. In the context of this model, a BLEU score of approximately 0.533 suggests a moderate level of accuracy in generating text that matches the reference sequences.

- **ROUGE Scores:** These scores evaluate the overlap between the generated text and the reference texts across several dimensions:
  - **F-measure:** Reflects the balance between precision (exactness) and recall (completeness) in matching the reference texts. The model achieves over 0.74 in ROUGE-L, indicating a good balance in capturing the essence of the target texts.
  - **Precision:** Indicates the proportion of correctly generated words against all generated words. With values above 0.75 for ROUGE-1, the model demonstrates a high level of precision.
  - **Recall:** Measures the proportion of correctly generated words against all words in the reference texts. The model shows strong recall rates, suggesting it can capture a significant portion of the reference content.

- **Test Loss:** 1.042. This value indicates the model's average error in predicting the target sequences during testing. Lower loss values denote better performance. ??The observed loss suggests the model has room for improvement but is on a promising trajectory??.




### Detailed Examples

Let's break down specific examples to illustrate how the model performs:

1. **Generated Text:** "He felt nervous getting in the water for the first time. Eventually, he got the hang of propelling himself in the water. Wendell became a great swimmer."
   
   **Decoded Target:** "He always felt nervous getting into the water, before his accident. Eventually, he probably would have got the hang of it. Wendell could have became a great swimmer, if it wasn't for that sad day in the pond."

   **Detailed Interpretation:** The model's output closely follows the narrative arc of the reference but optimistically concludes Wendell becoming a great swimmer, unlike the target text which hints at a tragic turn. This demonstrates the model's capability in narrative continuation but also highlights its challenge in capturing underlying tones or implied narratives.

2. **Generated Text:** "But they were from two different social backgrounds. They tried and tried to make their love work. But it just wasn't meant to be."
   
   **Decoded Target:** "They were from two different social backgrounds. They tried and tried to make their love work. But it just wasn't meant to be."

   **Detailed Interpretation:** Here, the model nearly perfectly replicates the target narrative, indicating a strong alignment in simpler, more direct narratives. The slight addition of "But" at the beginning of the generated text introduces a negligible deviation, showcasing the model's precision in simpler contexts.




### Comparative Analysis with Paper


### Takeway

The FLAN-T5 model demonstrates a commendable ability to generate text that aligns with the given context and targets, as evidenced by its BLEU and ROUGE scores. While there is room for optimization, particularly in enhancing the model's precision and reducing test loss, its current performance showcases a robust foundation for narrative generation tasks. Comparing these results with benchmarks from the literature can further elucidate the model's positioning within the field and guide future improvements.





