This page presnets the results of the experiments of the model flan-t5 and compare the results with the results generated by the  Paper Counterfactual Story Reasoning and Generation.


### Table of Contents

1. [**Introduction**](#Introduction)
  
2. [**Experiment Environment**](#Experiment-Environment)
   - [Model Configuration and Parameters](#Model-Configuration-and-Parameters)

3. [**Experiment 1: Initial Prototype**](#Experiment-1-Initial-Prototype)
   - [Results Table and Interpretation](#Model-Performance-Overview)
   - [Detailed Examples and Interpretation](#Detailed-Examples-and-Interpretation)

4. [**Experiment 2: Optimized Architecture**](#Experiment-2-Optimized-Architecture)
   - [Results Table and Interpretation](#Results-Table)
   - [Analysis of Differences - experiment 2 vs experiment 1 ](#Analysis-of-Differences-experiment2-vs-experiment1)

5. [**Experiment 3: Input Sequence Modification**](#Experiment-3-Input-Sequence-Modification)
   - [Results Table and Interpretation](#Results-Table-and-Comparison-with-Experiment-2)
   - [Analysis of Differences - experiment 3 vs experiment 2 ](#Analysis-of-Differences-experiment3-vs-experiment2)

6. [**General Assessment**](#General-Assessment)
   - [Analysis of Model Performance Across Experiments](#Analysis-of-Model-Performance-Across-Experiments)
   - [Insights and Future Directions](#Insights-and-Future-Directions)



# Introduction

The main objective of this experiment is to assess the efficacy of the flan-t5-base model. This assessment is initiated by utilizing well-established evaluation metrics such as BLEU and ROUGE, and then incorporates metrics that are more sensitive to context, such as BERT and BART. The data for this evaluation consists of stories that have undergone modifications to include a counterfactual scenario. The goal is to determine the model's proficiency in altering the story's conclusion to mirror the introduced counterfactual element. The dataset structure is as follows:

- **Premise**: The story's initial context (e.g., "Andrea wanted a picture of her jumping.").
- **Initial**: The event that precedes the counterfactual change (e.g., "She set the camera up.").
- **Counterfactual**: The introduced hypothetical change (e.g., "She asked her friend to draw one.").
- **Original Ending**: The conclusion following the initial scenario without the counterfactual change (e.g., "Then, she jumped in the air...").
- **Edited Ending**: The conclusion generated by the model that should account for the counterfactual change (e.g., "Then, she jumped in the air to demonstrate...").

```json
{
  "story_id": "4fd7d150-b080-4fb1-a592-8c27fa6e1fc8",
  "premise": "Andrea wanted a picture of her jumping.",
  "initial": "She set the camera up.",
  "counterfactual": "She asked her friend to draw one.",
  "original_ending": "Then, she jumped in the air. The picture kept coming out wrong. It took twenty tries to get it right.",
  "edited_ending": [
    "Then, she jumped in the air to demonstrate how she wanted it to look.",
    "The picture kept coming out wrong.",
    "It took drawing it several times to get it right."
  ]
}
```



# Experiment environment:

**Flan-T5**

  | Name  | Type                       | Params
-----------------------------------------------------
0 | model | T5ForConditionalGeneration | 247 M
-----------------------------------------------------
247 M     Trainable params
0         Non-trainable params
247 M     Total params
990.311   Total estimated model params size (MB)



## Experiment 1: Initial Prototype
(Wed 14/02/2023 - branch prototype-end_end). 

### Results Table and Interpretation

| Metric               | FLAN-T5 Test Results | Paper Benchmark | Interpretation                                         |
|----------------------|----------------------|---------------------------|--------------------------------------------------------|
| avg_bleu             | 0.533                | -                         | Moderate accuracy in text generation compared to targets. |
| rouge1_avg_fmeasure  | 0.754                | -                         | Good balance between precision and recall in capturing key information. |
| rouge2_avg_fmeasure  | 0.633                | -                         | Demonstrates the model's ability to replicate more complex sequences. |
| rougeL_avg_fmeasure  | 0.740                | -                         | High effectiveness in capturing longer sequences faithfully. |
| test_loss            | 1.042                | -                         | Indicates potential for improvement in prediction accuracy. |


- **BLEU Score (avg_bleu):** 0.533. This score measures the model's accuracy in reproducing the exact sequences of words in the target texts. A score closer to 1 indicates higher accuracy. In the context of this model, a BLEU score of approximately 0.533 suggests a moderate level of accuracy in generating text that matches the reference sequences.

- **ROUGE Scores:** These scores evaluate the overlap between the generated text and the reference texts across several dimensions:
  - **F-measure:** Reflects the balance between precision (exactness) and recall (completeness) in matching the reference texts. The model achieves over 0.74 in ROUGE-L, indicating a good balance in capturing the essence of the target texts.
  - **Precision:** Indicates the proportion of correctly generated words against all generated words. With values above 0.75 for ROUGE-1, the model demonstrates a high level of precision.
  - **Recall:** Measures the proportion of correctly generated words against all words in the reference texts. The model shows strong recall rates, suggesting it can capture a significant portion of the reference content.

- **Test Loss:** 1.042. This value indicates the model's average error in predicting the target sequences during testing. Lower loss values denote better performance. ??The observed loss suggests the model has room for improvement but is on a promising trajectory??.

### Detailed Examples and Interpretation

Let's break down specific examples to illustrate how the model performs:

1. **Generated Text:** "He felt nervous getting in the water for the first time. Eventually, he got the hang of propelling himself in the water. Wendell became a great swimmer."
   
   **Decoded Target:** "He always felt nervous getting into the water, before his accident. Eventually, he probably would have got the hang of it. Wendell could have became a great swimmer, if it wasn't for that sad day in the pond."

   **Detailed Interpretation:** The model's output closely follows the narrative arc of the reference but optimistically concludes Wendell becoming a great swimmer, unlike the target text which hints at a tragic turn. This demonstrates the model's capability in narrative continuation but also highlights its challenge in capturing underlying tones or implied narratives.

2. **Generated Text:** "But they were from two different social backgrounds. They tried and tried to make their love work. But it just wasn't meant to be."
   
   **Decoded Target:** "They were from two different social backgrounds. They tried and tried to make their love work. But it just wasn't meant to be."

   **Detailed Interpretation:** Here, the model nearly perfectly replicates the target narrative, indicating a strong alignment in simpler, more direct narratives. The slight addition of "But" at the beginning of the generated text introduces a negligible deviation, showcasing the model's precision in simpler contexts.


The FLAN-T5 model demonstrates a commendable ability to generate text that aligns with the given context and targets, as evidenced by its BLEU and ROUGE scores. While there is room for optimization, particularly in enhancing the model's precision and reducing test loss, its current performance showcases a robust foundation for narrative generation tasks. Comparing these results with benchmarks from the literature can further elucidate the model's positioning within the field and guide future improvements.

## Experiment 2: Optimized Architecture
(Mon 19/02/2024 - branch prototype-end_to_end)

We have optimised the architecture to handel the tokenisation and padding at the level of the preprocess and collate_fn functions.


### Results Table and Interpretation

| Metric                   | Score                 |
|--------------------------|-----------------------|
| Average BLEU Score       | 1.018                 |
| ROUGE-1 F-measure        | 2.279                 |
| ROUGE-1 Precision        | 3.172                 |
| ROUGE-1 Recall           | 1.812                 |
| ROUGE-2 F-measure        | 1.813                 |
| ROUGE-2 Precision        | 2.580                 |
| ROUGE-2 Recall           | 1.425                 |
| ROUGE-L F-measure        | 2.201                 |
| ROUGE-L Precision        | 3.063                 |
| ROUGE-L Recall           | 1.750                 |
| ROUGE-Lsum F-measure     | 2.201                 |
| ROUGE-Lsum Precision     | 3.063                 |
| ROUGE-Lsum Recall        | 1.750                 |
| Validation Loss          | 1.026                 |


- **Average BLEU Score:** 1.018, indicating a good level of accuracy in matching the reference texts. This score suggests that the model can reproduce specific sequences of words with a reasonable degree of accuracy.

- **ROUGE Metrics:**
  - **ROUGE-1 F-measure:** 2.279, showing the model's capability in capturing individual words from the reference texts.
  - **ROUGE-2 F-measure:** 1.813, indicating the model's effectiveness in capturing two-word phrases, a bit lower than single-word accuracy but still commendable.
  - **ROUGE-L F-measure:** 2.201, reflecting the model's ability to generate longer, coherent sequences similar to the reference texts.

- **Precision vs. Recall:** The model has higher precision than recall across the board, which means it tends to generate relevant content well but might not always capture the full breadth of the reference texts.

- **Validation Loss:** At 1.026, the model's error rate is moderate, suggesting room for improvement but also indicating a decent understanding of the text generation task.


### Analysis of Differences - experiment 2 vs experiment 1

The numerical data presents a compelling case for the positive impact of the architectural optimizations introduced in Experiment 2. Future iterations of the model will likely continue to build on these improvements, aiming for even higher levels of performance in narrative text generation tasks.

- **BLEU Score Improvement:** The BLEU score saw an increase of approximately 92%, moving from 0.533 to 1.018. This significant jump indicates a near-doubling of accuracy in text generation compared to the reference texts.

- **Enhancements in ROUGE Metrics:** All ROUGE metrics observed substantial improvements. The ROUGE-1 F-measure increased by more than 202%, ROUGE-2 by approximately 186%, and ROUGE-L by nearly 198%. These metrics suggest a marked enhancement in the model's ability to replicate both the micro (words and two-word phrases) and macro (longer sequences) elements of the reference texts accurately.

- **Precision and Recall Analysis:** There is a considerable increase in ROUGE metrics for Experiment 2 implies improvements in both the relevance and completeness of the generated text relative to the references.

- **Reduction in Validation Loss:** The validation loss saw a slight decrease from 1.042 to 1.026. Although this change is modest, it still represents a positive shift towards reducing the model's average prediction error.



## Experiment 3: Input Sequence Modification
(Tues 20/02/2014 - branch prototype-end_to_end )

In this experiement we have changed `input_sequence` from the paper format ( premise, initial, orginal_ending, </s>, premise, couterfactual) to the format (premise, initial, original_ending, counterfactual)

### Results Table and Interpretation

| Metric                   | Experiment 2 Score    | Experiment 3 Score    | Difference          |
|--------------------------|-----------------------|-----------------------|---------------------|
| Average BLEU Score       | 1.018                 | 1.0175716876983643    | -0.0004283123016357 |
| ROUGE-1 F-measure        | 2.279                 | 2.2779605388641357    | -0.0010394611358643 |
| ROUGE-1 Precision        | 3.172                 | 3.1704189777374268    | -0.0015810222625732 |
| ROUGE-1 Recall           | 1.812                 | 1.81069815158844      | -0.00130184841156   |
| ROUGE-2 F-measure        | 1.813                 | 1.8134219646453857    |  0.0004219646453857 |
| ROUGE-2 Precision        | 2.580                 | 2.5816280841827393    |  0.0016280841827393 |
| ROUGE-2 Recall           | 1.425                 | 1.424911379814148     | -0.000088620185852  |
| ROUGE-L F-measure        | 2.201                 | 2.2031939029693604    |  0.0021939029693604 |
| ROUGE-L Precision        | 3.063                 | 3.0661745071411133    |  0.0031745071411133 |
| ROUGE-L Recall           | 1.750                 | 1.7513031959533691    |  0.0013031959533691 |
| ROUGE-Lsum F-measure     | 2.201                 | 2.2031939029693604    |  0.0021939029693604 |
| ROUGE-Lsum Precision     | 3.063                 | 3.0661745071411133    |  0.0031745071411133 |
| ROUGE-Lsum Recall        | 1.750                 | 1.7513031959533691    |  0.0013031959533691 |
| Validation Loss          | 1.026                 | 1.0790570974349976    |  0.0530570974349976 |

### Analysis of Differences - experiement 3 vs experiment 2

- **Average BLEU Score:** The difference is negligible, indicating that the change in input sequence format had an almost imperceptible impact on the model's accuracy.

- **ROUGE Scores:** The differences in ROUGE scores are minimal across the board. The slight variations observed (some positive, some negative) suggest minor shifts in the model's ability to capture both the individual words and longer coherent sequences of the reference texts. However, these shifts are so small that they might not be significant in practice.

- **Validation Loss:** The increase in validation loss is the most notable difference, suggesting a slight reduction in model performance with the modified input sequence format. While the absolute difference is small, it's the most significant change observed between the experiments, potentially indicating the separator token's role in model efficiency or understanding.

The table and analysis illustrate that the changes made in Experiment 3's input sequence format had minimal impact on most performance metrics, with the exception of a somewhat more pronounced increase in validation loss. This provides some insight into the effects of input formatting used in the original paper on the model performance.

## Experiment 4 (28/02)

The metrics selected for Experiment 4 are:

- A standard metric (BLEU, ROUGE, BERTScore; any version) with the following guidelines:

BLEU(prediction, edited_ending): Higher scores are preferred.
BLEU(prediction, counterfactual): Higher scores are preferred.
BLEU(prediction, initial): Lower scores are preferred.
BLEU(prediction, original_ending): Lower scores are preferred (though the strings are lengthy and may still be somewhat similar for effective predictions).

A composite metric can be derived by calculating the difference between the desired and undesired scores.

To validate these hypotheses, the following metrics will be calculated and reported for the dataset:
BLEU(edited_ending, counterfactual): Higher scores are preferred.
BLEU(edited_ending, initial): Lower scores are preferred.
BLEU(edited_ending, original_ending): Lower scores are preferred (although the long strings may result in similarities).

It's hypothesized that some of these measures could serve as an "upper limit" for the model's prediction scores, although this remains uncertain. For example, it might be presumed that BLEU(edited_ending, counterfactual) will always exceed BLEU(prediction, counterfactual), but this is not guaranteed. Ideally, their scores should be comparable.

- BARTScore:
The considerations mentioned above also apply to BARTScore. The same arguments are provided in the required format, with identical trends expected.

