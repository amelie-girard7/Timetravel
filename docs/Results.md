# Experiment1: Training the model with six examples only

## Model Evaluation Report

In our project, we aimed to train a machine learning model capable of generating edited story endings based on given story premises, initial parts, original endings, and counterfactual scenarios. To assess the performance of our model, we employed two sets of metrics: BLEU and ROUGE. These metrics help us understand how closely the model-generated texts match the expected output. Below, we provide an interpretation of these metrics using examples from our logs, designed for readers unfamiliar with these terms.

### Understanding BLEU Score

The BLEU (Bilingual Evaluation Understudy) score is a measure used primarily in machine translation to evaluate the quality of text generated by the model against a set of reference texts. It calculates the precision of n-grams (a sequence of n words) in the generated text that are also present in the reference text. A higher BLEU score indicates a greater overlap, suggesting better performance.

- **Example from Logs:**
  - **Average BLEU Score:** 0.0705
  - This score suggests that there is a 7.05% overlap between the n-grams in our model's generated texts and those in the reference texts. While this is a simplified interpretation, it generally indicates that our model has room for improvement in accurately replicating the reference texts' structure and content.

### Understanding ROUGE Metrics

ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics are used to evaluate the quality of summaries or text generation tasks against a set of reference summaries or texts. There are several ROUGE metrics, each providing different insights:

1. **ROUGE-1:** Measures the overlap of unigrams (single words) between the generated and reference texts.
   - **Precision:** 0.4428 (44.28% of the unigrams in the generated text are in the reference text)
   - **Recall:** 0.2333 (23.33% of the unigrams in the reference text are captured in the generated text)
   - **F-measure:** 0.2912 (A harmonic mean of precision and recall, suggesting the overall balance between them)

2. **ROUGE-2:** Measures the overlap of bigrams (sequences of two words) between the generated and reference texts.
   - **Precision:** 0.1270
   - **Recall:** 0.0939
   - **F-measure:** 0.1079

3. **ROUGE-L:** Focuses on the longest common subsequence, considering the order of words.
   - **Precision:** 0.3864
   - **Recall:** 0.2045
   - **F-measure:** 0.2541

4. **ROUGE-Lsum:** Similar to ROUGE-L but applied to the sum of the longest common subsequences for each segment of the generated and reference texts.
   - Shares the same scores as ROUGE-L in this context.

### Interpretation with Example

Let's consider the generated text and target text for a clearer understanding:

- **Generated Text:** "She gathered the ingredients and propped open the cookbook."
- **Target Text:** "A breeze turned the page when she wasn't looking. Reading the new recipe, Sue decided it would be more suitable. She was thrilled to learn that she had found a new recipe!"

When comparing these texts:
- The **BLEU score** suggests a low overall n-gram overlap, indicating the generated text misses much of the content and structure found in the target.
- The **ROUGE-1 scores** show that while the model captures some of the individual words from the target (precision), it misses a significant portion of the content (recall), leading to a moderate F-measure.
- The **ROUGE-2 and ROUGE-L scores** further highlight the model's challenge in capturing sequences of words and maintaining the order found in the target text.

### Conclusion

The evaluation metrics indicate that our model has managed to replicate some aspects of the target texts but still struggles to accurately and comprehensively generate text that matches the reference texts closely. Improvements could be made by refining the model's architecture, training on more diverse datasets, or tweaking the generation process to better capture the nuances of the target texts.

# Experiment1: Training the model with 100 examples 