{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Configuring to use GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 Config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /data/agirard/Projects/Timetravel/models/model_2024-05-15-13/training_logs\n",
      "/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory /data/agirard/Projects/Timetravel/models/model_2024-05-15-13 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 247 M \n",
      "-----------------------------------------------------\n",
      "247 M     Trainable params\n",
      "0         Non-trainable params\n",
      "247 M     Total params\n",
      "990.311   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file: /data/agirard/Projects/Timetravel/data/transformed/train_supervised_small_sample.json\n",
      "Checking file: /data/agirard/Projects/Timetravel/data/transformed/dev_data_sample.json\n",
      "Checking file: /data/agirard/Projects/Timetravel/data/transformed/test_data_sample.json\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/agirard/Projects/Timetravel/src/utils/utils.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  differential_weights_tensors = [torch.tensor(dw, dtype=torch.float).to(input_ids_padded.device) for dw in differential_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]The forward outputs odict_keys(['loss', 'logits', 'past_key_values', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_attentions'])\n",
      "No attentions were returned. Check model configuration.\n",
      "No attentions were returned. Check model configuration.\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "/data/agirard/Projects/Timetravel/src/utils/utils.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  differential_weights_tensors = [torch.tensor(dw, dtype=torch.float).to(input_ids_padded.device) for dw in differential_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00,  6.00it/s, v_num=0, train_loss_step=14.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/agirard/Projects/Timetravel/src/utils/utils.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  differential_weights_tensors = [torch.tensor(dw, dtype=torch.float).to(input_ids_padded.device) for dw in differential_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The forward outputs odict_keys(['loss', 'logits', 'past_key_values', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_attentions'])\n",
      "No attentions were returned. Check model configuration.\n",
      "No attentions were returned. Check model configuration.\n",
      "Epoch 0: 100%|██████████| 1/1 [00:01<00:00,  0.76it/s, v_num=0, train_loss_step=14.30, val_loss=8.230, avg_val_loss=8.230, bleu_prediction_edited=0.248, bleu_prediction_cf=2.490, bleu_prediction_initial=2.790, bleu_prediction_original=0.211, bleu_edited_ending_cf=0.000, bleu_edited_ending_initial=0.000, bleu_edited_ending_original=0.000, rouge_prediction_edited_rouge-1_f=0.149, rouge_prediction_edited_rouge-1_p=0.417, rouge_prediction_edited_rouge-1_r=0.0913, rouge_prediction_edited_rouge-2_f=0.0294, rouge_prediction_edited_rouge-2_p=0.100, rouge_prediction_edited_rouge-2_r=0.0172, rouge_prediction_edited_rouge-l_f=0.149, rouge_prediction_edited_rouge-l_p=0.417, rouge_prediction_edited_rouge-l_r=0.0913, rouge_prediction_cf_rouge-1_f=0.100, rouge_prediction_cf_rouge-1_p=0.167, rouge_prediction_cf_rouge-1_r=0.0714, rouge_prediction_cf_rouge-2_f=0.000, rouge_prediction_cf_rouge-2_p=0.000, rouge_prediction_cf_rouge-2_r=0.000, rouge_prediction_cf_rouge-l_f=0.100, rouge_prediction_cf_rouge-l_p=0.167, rouge_prediction_cf_rouge-l_r=0.0714, rouge_prediction_initial_rouge-1_f=0.000, rouge_prediction_initial_rouge-1_p=0.000, rouge_prediction_initial_rouge-1_r=0.000, rouge_prediction_initial_rouge-2_f=0.000, rouge_prediction_initial_rouge-2_p=0.000, rouge_prediction_initial_rouge-2_r=0.000, rouge_prediction_initial_rouge-l_f=0.000, rouge_prediction_initial_rouge-l_p=0.000, rouge_prediction_initial_rouge-l_r=0.000, rouge_prediction_original_rouge-1_f=0.138, rouge_prediction_original_rouge-1_p=0.333, rouge_prediction_original_rouge-1_r=0.087, rouge_prediction_original_rouge-2_f=0.000, rouge_prediction_original_rouge-2_p=0.000, rouge_prediction_original_rouge-2_r=0.000, rouge_prediction_original_rouge-l_f=0.138, rouge_prediction_original_rouge-l_p=0.333, rouge_prediction_original_rouge-l_r=0.087, rouge_edited_ending_cf_rouge-1_f=0.284, rouge_edited_ending_cf_rouge-1_p=0.214, rouge_edited_ending_cf_rouge-1_r=0.429, rouge_edited_ending_cf_rouge-2_f=0.000, rouge_edited_ending_cf_rouge-2_p=0.000, rouge_edited_ending_cf_rouge-2_r=0.000, rouge_edited_ending_cf_rouge-l_f=0.193, rouge_edited_ending_cf_rouge-l_p=0.147, rouge_edited_ending_cf_rouge-l_r=0.286, rouge_edited_ending_initial_rouge-1_f=0.283, rouge_edited_ending_initial_rouge-1_p=0.198, rouge_edited_ending_initial_rouge-1_r=0.500, rouge_edited_ending_initial_rouge-2_f=0.0942, rouge_edited_ending_initial_rouge-2_p=0.0639, rouge_edited_ending_initial_rouge-2_r=0.182, rouge_edited_ending_initial_rouge-l_f=0.204, rouge_edited_ending_initial_rouge-l_p=0.142, rouge_edited_ending_initial_rouge-l_r=0.364, rouge_edited_ending_original_rouge-1_f=0.759, rouge_edited_ending_original_rouge-1_p=0.692, rouge_edited_ending_original_rouge-1_r=0.848, rouge_edited_ending_original_rouge-2_f=0.681, rouge_edited_ending_original_rouge-2_p=0.613, rouge_edited_ending_original_rouge-2_r=0.778, rouge_edited_ending_original_rouge-l_f=0.759, rouge_edited_ending_original_rouge-l_p=0.692, rouge_edited_ending_original_rouge-l_r=0.848, bert_prediction_edited_precision=0.688, bert_prediction_edited_recall=0.485, bert_prediction_edited_f1=0.569, bert_prediction_cf_precision=0.681, bert_prediction_cf_recall=0.580, bert_prediction_cf_f1=0.626, bert_prediction_initial_precision=0.624, bert_prediction_initial_recall=0.594, bert_prediction_initial_f1=0.608, bert_prediction_original_precision=0.645, bert_prediction_original_recall=0.502, bert_prediction_original_f1=0.564, bert_edited_ending_cf_precision=0.521, bert_edited_ending_cf_recall=0.617, bert_edited_ending_cf_f1=0.565, bert_edited_ending_initial_precision=0.533, bert_edited_ending_initial_recall=0.698, bert_edited_ending_initial_f1=0.604, bert_edited_ending_original_precision=0.775, bert_edited_ending_original_recall=0.846, bert_edited_ending_original_f1=0.808, bart_prediction_edited_avg_score=-2.94, bart_prediction_cf_avg_score=-3.58, bart_prediction_initial_avg_score=-3.65, bart_prediction_original_avg_score=-3.09, bart_edited_ending_cf_avg_score=-3.16, bart_edited_ending_initial_avg_score=-3.18, bart_edited_ending_original_avg_score=-1.07, train_loss_epoch=14.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1: 'val_loss' reached 8.22616 (best 8.22616), saving model to '/data/agirard/Projects/Timetravel/models/model_2024-05-15-13/checkpoint-epoch=00-val_loss=8.23.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:10<00:00,  0.10it/s, v_num=0, train_loss_step=14.30, val_loss=8.230, avg_val_loss=8.230, bleu_prediction_edited=0.248, bleu_prediction_cf=2.490, bleu_prediction_initial=2.790, bleu_prediction_original=0.211, bleu_edited_ending_cf=0.000, bleu_edited_ending_initial=0.000, bleu_edited_ending_original=0.000, rouge_prediction_edited_rouge-1_f=0.149, rouge_prediction_edited_rouge-1_p=0.417, rouge_prediction_edited_rouge-1_r=0.0913, rouge_prediction_edited_rouge-2_f=0.0294, rouge_prediction_edited_rouge-2_p=0.100, rouge_prediction_edited_rouge-2_r=0.0172, rouge_prediction_edited_rouge-l_f=0.149, rouge_prediction_edited_rouge-l_p=0.417, rouge_prediction_edited_rouge-l_r=0.0913, rouge_prediction_cf_rouge-1_f=0.100, rouge_prediction_cf_rouge-1_p=0.167, rouge_prediction_cf_rouge-1_r=0.0714, rouge_prediction_cf_rouge-2_f=0.000, rouge_prediction_cf_rouge-2_p=0.000, rouge_prediction_cf_rouge-2_r=0.000, rouge_prediction_cf_rouge-l_f=0.100, rouge_prediction_cf_rouge-l_p=0.167, rouge_prediction_cf_rouge-l_r=0.0714, rouge_prediction_initial_rouge-1_f=0.000, rouge_prediction_initial_rouge-1_p=0.000, rouge_prediction_initial_rouge-1_r=0.000, rouge_prediction_initial_rouge-2_f=0.000, rouge_prediction_initial_rouge-2_p=0.000, rouge_prediction_initial_rouge-2_r=0.000, rouge_prediction_initial_rouge-l_f=0.000, rouge_prediction_initial_rouge-l_p=0.000, rouge_prediction_initial_rouge-l_r=0.000, rouge_prediction_original_rouge-1_f=0.138, rouge_prediction_original_rouge-1_p=0.333, rouge_prediction_original_rouge-1_r=0.087, rouge_prediction_original_rouge-2_f=0.000, rouge_prediction_original_rouge-2_p=0.000, rouge_prediction_original_rouge-2_r=0.000, rouge_prediction_original_rouge-l_f=0.138, rouge_prediction_original_rouge-l_p=0.333, rouge_prediction_original_rouge-l_r=0.087, rouge_edited_ending_cf_rouge-1_f=0.284, rouge_edited_ending_cf_rouge-1_p=0.214, rouge_edited_ending_cf_rouge-1_r=0.429, rouge_edited_ending_cf_rouge-2_f=0.000, rouge_edited_ending_cf_rouge-2_p=0.000, rouge_edited_ending_cf_rouge-2_r=0.000, rouge_edited_ending_cf_rouge-l_f=0.193, rouge_edited_ending_cf_rouge-l_p=0.147, rouge_edited_ending_cf_rouge-l_r=0.286, rouge_edited_ending_initial_rouge-1_f=0.283, rouge_edited_ending_initial_rouge-1_p=0.198, rouge_edited_ending_initial_rouge-1_r=0.500, rouge_edited_ending_initial_rouge-2_f=0.0942, rouge_edited_ending_initial_rouge-2_p=0.0639, rouge_edited_ending_initial_rouge-2_r=0.182, rouge_edited_ending_initial_rouge-l_f=0.204, rouge_edited_ending_initial_rouge-l_p=0.142, rouge_edited_ending_initial_rouge-l_r=0.364, rouge_edited_ending_original_rouge-1_f=0.759, rouge_edited_ending_original_rouge-1_p=0.692, rouge_edited_ending_original_rouge-1_r=0.848, rouge_edited_ending_original_rouge-2_f=0.681, rouge_edited_ending_original_rouge-2_p=0.613, rouge_edited_ending_original_rouge-2_r=0.778, rouge_edited_ending_original_rouge-l_f=0.759, rouge_edited_ending_original_rouge-l_p=0.692, rouge_edited_ending_original_rouge-l_r=0.848, bert_prediction_edited_precision=0.688, bert_prediction_edited_recall=0.485, bert_prediction_edited_f1=0.569, bert_prediction_cf_precision=0.681, bert_prediction_cf_recall=0.580, bert_prediction_cf_f1=0.626, bert_prediction_initial_precision=0.624, bert_prediction_initial_recall=0.594, bert_prediction_initial_f1=0.608, bert_prediction_original_precision=0.645, bert_prediction_original_recall=0.502, bert_prediction_original_f1=0.564, bert_edited_ending_cf_precision=0.521, bert_edited_ending_cf_recall=0.617, bert_edited_ending_cf_f1=0.565, bert_edited_ending_initial_precision=0.533, bert_edited_ending_initial_recall=0.698, bert_edited_ending_initial_f1=0.604, bert_edited_ending_original_precision=0.775, bert_edited_ending_original_recall=0.846, bert_edited_ending_original_f1=0.808, bart_prediction_edited_avg_score=-2.94, bart_prediction_cf_avg_score=-3.58, bart_prediction_initial_avg_score=-3.65, bart_prediction_original_avg_score=-3.09, bart_edited_ending_cf_avg_score=-3.16, bart_edited_ending_initial_avg_score=-3.18, bart_edited_ending_original_avg_score=-1.07, train_loss_epoch=14.30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/agirard/Projects/Timetravel/src/utils/utils.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  differential_weights_tensors = [torch.tensor(dw, dtype=torch.float).to(input_ids_padded.device) for dw in differential_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]The forward outputs odict_keys(['loss', 'logits', 'past_key_values', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_attentions'])\n",
      "No attentions were returned. Check model configuration.\n",
      "No attentions were returned. Check model configuration.\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "             Test metric                           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "             avg_val_loss                       1.638059377670288\n",
      "   bart_edited_ending_cf_avg_score              -2.972923517227173\n",
      " bart_edited_ending_initial_avg_score          -2.7492918968200684\n",
      "bart_edited_ending_original_avg_score          -1.2895852327346802\n",
      "     bart_prediction_cf_avg_score               -2.150461435317993\n",
      "   bart_prediction_edited_avg_score             -3.546156406402588\n",
      "  bart_prediction_initial_avg_score             -2.91536021232605\n",
      "  bart_prediction_original_avg_score            -3.725609064102173\n",
      "       bert_edited_ending_cf_f1                 0.623887300491333\n",
      "   bert_edited_ending_cf_precision              0.5704318881034851\n",
      "     bert_edited_ending_cf_recall               0.6884160041809082\n",
      "    bert_edited_ending_initial_f1               0.5932033061981201\n",
      " bert_edited_ending_initial_precision           0.5741724371910095\n",
      "  bert_edited_ending_initial_recall             0.6135692000389099\n",
      "    bert_edited_ending_original_f1              0.8468313217163086\n",
      "bert_edited_ending_original_precision           0.8542401790618896\n",
      "  bert_edited_ending_original_recall            0.839550256729126\n",
      "        bert_prediction_cf_f1                   0.8704600930213928\n",
      "     bert_prediction_cf_precision               0.8859370350837708\n",
      "      bert_prediction_cf_recall                 0.855514645576477\n",
      "      bert_prediction_edited_f1                 0.6079882979393005\n",
      "   bert_prediction_edited_precision             0.672055721282959\n",
      "    bert_prediction_edited_recall               0.5550786852836609\n",
      "      bert_prediction_initial_f1                0.6851958632469177\n",
      "  bert_prediction_initial_precision             0.7386659979820251\n",
      "    bert_prediction_initial_recall              0.6389443874359131\n",
      "     bert_prediction_original_f1                0.6065242290496826\n",
      "  bert_prediction_original_precision            0.6765986680984497\n",
      "   bert_prediction_original_recall              0.5496026277542114\n",
      "        bleu_edited_ending_cf                          0.0\n",
      "      bleu_edited_ending_initial                       0.0\n",
      "     bleu_edited_ending_original                       0.0\n",
      "          bleu_prediction_cf                    12.862534523010254\n",
      "        bleu_prediction_edited                  1.099776029586792\n",
      "       bleu_prediction_initial                  2.4159653186798096\n",
      "       bleu_prediction_original                 0.9705489277839661\n",
      "   rouge_edited_ending_cf_rouge-1_f            0.20000000298023224\n",
      "   rouge_edited_ending_cf_rouge-1_p            0.13636364042758942\n",
      "   rouge_edited_ending_cf_rouge-1_r                   0.375\n",
      "   rouge_edited_ending_cf_rouge-2_f            0.05882352590560913\n",
      "   rouge_edited_ending_cf_rouge-2_p            0.03846153989434242\n",
      "   rouge_edited_ending_cf_rouge-2_r                   0.125\n",
      "   rouge_edited_ending_cf_rouge-l_f            0.13333332538604736\n",
      "   rouge_edited_ending_cf_rouge-l_p            0.09090909361839294\n",
      "   rouge_edited_ending_cf_rouge-l_r                    0.25\n",
      "rouge_edited_ending_initial_rouge-1_f           0.0277777761220932\n",
      "rouge_edited_ending_initial_rouge-1_p          0.022727273404598236\n",
      "rouge_edited_ending_initial_rouge-1_r           0.0357142873108387\n",
      "rouge_edited_ending_initial_rouge-2_f                  0.0\n",
      "rouge_edited_ending_initial_rouge-2_p                  0.0\n",
      "rouge_edited_ending_initial_rouge-2_r                  0.0\n",
      "rouge_edited_ending_initial_rouge-l_f           0.0277777761220932\n",
      "rouge_edited_ending_initial_rouge-l_p          0.022727273404598236\n",
      "rouge_edited_ending_initial_rouge-l_r           0.0357142873108387\n",
      "rouge_edited_ending_original_rouge-1_f          0.8666666746139526\n",
      "rouge_edited_ending_original_rouge-1_p          0.8863636255264282\n",
      "rouge_edited_ending_original_rouge-1_r          0.8478260636329651\n",
      "rouge_edited_ending_original_rouge-2_f          0.7735849022865295\n",
      "rouge_edited_ending_original_rouge-2_p          0.7884615659713745\n",
      "rouge_edited_ending_original_rouge-2_r          0.7592592835426331\n",
      "rouge_edited_ending_original_rouge-l_f          0.8666666746139526\n",
      "rouge_edited_ending_original_rouge-l_p          0.8863636255264282\n",
      "rouge_edited_ending_original_rouge-l_r          0.8478260636329651\n",
      "    rouge_prediction_cf_rouge-1_f               0.5333333015441895\n",
      "    rouge_prediction_cf_rouge-1_p               0.5714285969734192\n",
      "    rouge_prediction_cf_rouge-1_r                      0.5\n",
      "    rouge_prediction_cf_rouge-2_f               0.2857142686843872\n",
      "    rouge_prediction_cf_rouge-2_p               0.3333333432674408\n",
      "    rouge_prediction_cf_rouge-2_r                      0.25\n",
      "    rouge_prediction_cf_rouge-l_f               0.5333333015441895\n",
      "    rouge_prediction_cf_rouge-l_p               0.5714285969734192\n",
      "    rouge_prediction_cf_rouge-l_r                      0.5\n",
      "  rouge_prediction_edited_rouge-1_f            0.20689654350280762\n",
      "  rouge_prediction_edited_rouge-1_p             0.4285714328289032\n",
      "  rouge_prediction_edited_rouge-1_r            0.13636364042758942\n",
      "  rouge_prediction_edited_rouge-2_f             0.0624999962747097\n",
      "  rouge_prediction_edited_rouge-2_p             0.1666666716337204\n",
      "  rouge_prediction_edited_rouge-2_r            0.03846153989434242\n",
      "  rouge_prediction_edited_rouge-l_f            0.20689654350280762\n",
      "  rouge_prediction_edited_rouge-l_p             0.4285714328289032\n",
      "  rouge_prediction_edited_rouge-l_r            0.13636364042758942\n",
      "  rouge_prediction_initial_rouge-1_f            0.0952380895614624\n",
      "  rouge_prediction_initial_rouge-1_p            0.1428571492433548\n",
      "  rouge_prediction_initial_rouge-1_r            0.0714285746216774\n",
      "  rouge_prediction_initial_rouge-2_f                   0.0\n",
      "  rouge_prediction_initial_rouge-2_p                   0.0\n",
      "  rouge_prediction_initial_rouge-2_r                   0.0\n",
      "  rouge_prediction_initial_rouge-l_f            0.0952380895614624\n",
      "  rouge_prediction_initial_rouge-l_p            0.1428571492433548\n",
      "  rouge_prediction_initial_rouge-l_r            0.0714285746216774\n",
      " rouge_prediction_original_rouge-1_f           0.20000000298023224\n",
      " rouge_prediction_original_rouge-1_p            0.4285714328289032\n",
      " rouge_prediction_original_rouge-1_r            0.1304347813129425\n",
      " rouge_prediction_original_rouge-2_f           0.060606058686971664\n",
      " rouge_prediction_original_rouge-2_p            0.1666666716337204\n",
      " rouge_prediction_original_rouge-2_r           0.03703703731298447\n",
      " rouge_prediction_original_rouge-l_f           0.20000000298023224\n",
      " rouge_prediction_original_rouge-l_p            0.4285714328289032\n",
      " rouge_prediction_original_rouge-l_r            0.1304347813129425\n",
      "               val_loss                         1.638059377670288\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from src.models.model_T5 import FlanT5FineTuner\n",
    "from src.data_loader import create_dataloaders\n",
    "from src.utils.config import CONFIG\n",
    "from bertviz import model_view\n",
    "\n",
    "# Set the specific GPU to use\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Adjust the index to select a different GPU\n",
    "\n",
    "# Check if CUDA is available and print the device\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Configuring to use GPU.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "def setup_trainer(model_dir):\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=model_dir,\n",
    "        filename='checkpoint-{epoch:02d}-{val_loss:.2f}',\n",
    "        save_top_k=1,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    tensorboard_logger = TensorBoardLogger(save_dir=model_dir, name=\"training_logs\")\n",
    "    trainer = Trainer(\n",
    "            max_epochs=CONFIG[\"max_epochs\"],\n",
    "            accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "            devices=1 if torch.cuda.is_available() else None,\n",
    "            callbacks=[checkpoint_callback],\n",
    "            logger=tensorboard_logger\n",
    "        )\n",
    "    return trainer\n",
    "\n",
    "def setup_model(model_dir):\n",
    "    model = FlanT5FineTuner(CONFIG[\"model_name\"], model_dir)\n",
    "    model = model.to(device)  # Explicitly move your model to the correct device\n",
    "    return model\n",
    "\n",
    "def setup_dataloaders(model, tokenizer):\n",
    "    data_path = CONFIG[\"data_dir\"] / 'transformed'\n",
    "    batch_size = CONFIG[\"batch_size\"]\n",
    "    num_workers = CONFIG[\"num_workers\"]\n",
    "\n",
    "    dataloaders = create_dataloaders(data_path, tokenizer, batch_size, num_workers)\n",
    "    return dataloaders\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Directory for models and logs\n",
    "model_timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H\")\n",
    "model_dir = Path(CONFIG[\"models_dir\"]) / f\"model_{model_timestamp}\"\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Setup Model, Trainer, and Dataloaders\n",
    "tokenizer = T5Tokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "model = setup_model(model_dir)\n",
    "dataloaders = setup_dataloaders(model, tokenizer)\n",
    "trainer = setup_trainer(model_dir)\n",
    "\n",
    "# Training\n",
    "try:\n",
    "    trainer.fit(model, dataloaders['train_supervised_small_sample'], dataloaders['dev_data_sample'])\n",
    "    trainer.test(model, dataloaders['test_data_sample'])\n",
    "except Exception as e:\n",
    "    logger.exception(\"An error occurred during training or testing.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c661e5957d856bb83db7ea42c976ce980a0fafd4065ab3c3a672e7d1ad452dd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
