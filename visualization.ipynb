{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Configuring to use GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file: /data/agirard/Projects/Timetravel/data/transformed/train_supervised_small_sample.json\n",
      "Checking file: /data/agirard/Projects/Timetravel/data/transformed/dev_data_sample.json\n",
      "Checking file: /data/agirard/Projects/Timetravel/data/transformed/test_data_sample.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory /data/agirard/Projects/Timetravel/models/model_2024-05-21-14 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 247 M \n",
      "-----------------------------------------------------\n",
      "247 M     Trainable params\n",
      "0         Non-trainable params\n",
      "247 M     Total params\n",
      "990.311   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:An error occurred during training or testing.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1466926/1528999539.py\", line 79, in <module>\n",
      "    trainer.fit(model, dataloaders['train_supervised_small_sample'], dataloaders['dev_data_sample'])\n",
      "  File \"/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 544, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 989, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 1033, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 1062, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py\", line 182, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 134, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 391, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py\", line 309, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py\", line 403, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/agirard/Projects/Timetravel/src/models/model_T5.py\", line 161, in validation_step\n",
      "    generated_texts = self.generate_text(\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/agirard/Projects/Timetravel/src/models/model_T5.py\", line 497, in generate_text\n",
      "    attentions = generated_ids.attentions\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'GenerateEncoderDecoderOutput' object has no attribute 'attentions'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 89\u001b[0m\n\u001b[1;32m     86\u001b[0m input_ids \u001b[39m=\u001b[39m input_batch[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     87\u001b[0m attention_mask \u001b[39m=\u001b[39m input_batch[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 89\u001b[0m generated_texts, attentions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate_text(input_ids, attention_mask)\n\u001b[1;32m     91\u001b[0m \u001b[39m# Visualize attention\u001b[39;00m\n\u001b[1;32m     92\u001b[0m model\u001b[39m.\u001b[39mvisualize_attention_bertviz(input_ids, attention_mask, input_batch[\u001b[39m'\u001b[39m\u001b[39moriginal_ending\u001b[39m\u001b[39m'\u001b[39m], attentions)\n",
      "File \u001b[0;32m/data/agirard/Projects/Timetravel/src/models/model_T5.py:488\u001b[0m, in \u001b[0;36mFlanT5FineTuner.generate_text\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_text\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask):\n\u001b[1;32m    484\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[39m    Generates text sequences from the provided input components using the model,\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[39m    with a customizable maximum length for the generated text.\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 488\u001b[0m     generated_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m    489\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids, \n\u001b[1;32m    490\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask, \n\u001b[1;32m    491\u001b[0m     max_length\u001b[39m=\u001b[39mCONFIG[\u001b[39m\"\u001b[39m\u001b[39mmax_gen_length\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    492\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,  \u001b[39m# Ensure attentions are included in the outputs\u001b[39;00m\n\u001b[1;32m    493\u001b[0m     return_dict_in_generate\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m  \u001b[39m# Ensure outputs are returned as a dictionary\u001b[39;00m\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    496\u001b[0m     \u001b[39m# Extracting attentions\u001b[39;00m\n\u001b[1;32m    497\u001b[0m     attentions \u001b[39m=\u001b[39m generated_ids\u001b[39m.\u001b[39mattentions\n",
      "File \u001b[0;32m/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/transformers/generation/utils.py:1370\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m   1363\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1364\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration results, please set `padding_side=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` when initializing the tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1365\u001b[0m         )\n\u001b[1;32m   1367\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1368\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1369\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1370\u001b[0m     model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_encoder_decoder_kwargs_for_generation(\n\u001b[1;32m   1371\u001b[0m         inputs_tensor, model_kwargs, model_input_name\n\u001b[1;32m   1372\u001b[0m     )\n\u001b[1;32m   1374\u001b[0m \u001b[39m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/transformers/generation/utils.py:491\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    489\u001b[0m encoder_kwargs[\u001b[39m\"\u001b[39m\u001b[39mreturn_dict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    490\u001b[0m encoder_kwargs[model_input_name] \u001b[39m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 491\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m]: ModelOutput \u001b[39m=\u001b[39m encoder(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoder_kwargs)\n\u001b[1;32m    493\u001b[0m \u001b[39mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1016\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1015\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to initialize the model with valid token embeddings\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1016\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens(input_ids)\n\u001b[1;32m   1018\u001b[0m batch_size, seq_length \u001b[39m=\u001b[39m input_shape\n\u001b[1;32m   1020\u001b[0m \u001b[39m# required mask seq length can be calculated via length of past\u001b[39;00m\n",
      "File \u001b[0;32m/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39membedding(\n\u001b[1;32m    164\u001b[0m         \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_norm,\n\u001b[1;32m    165\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_type, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_grad_by_freq, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparse)\n",
      "File \u001b[0;32m/data/agirard/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39membedding(weight, \u001b[39minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from src.models.model_T5 import FlanT5FineTuner\n",
    "from src.data_loader import create_dataloaders\n",
    "from src.utils.config import CONFIG\n",
    "from bertviz import model_view, head_view\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the specific GPU to use\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Adjust the index to select a different GPU\n",
    "\n",
    "# Check if CUDA is available and print the device\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Configuring to use GPU.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "def setup_trainer(model_dir):\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=model_dir,\n",
    "        filename='checkpoint-{epoch:02d}-{val_loss:.2f}',\n",
    "        save_top_k=1,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    tensorboard_logger = TensorBoardLogger(save_dir=model_dir, name=\"training_logs\")\n",
    "    trainer = Trainer(\n",
    "            max_epochs=CONFIG[\"max_epochs\"],\n",
    "            accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "            devices=1 if torch.cuda.is_available() else None,\n",
    "            callbacks=[checkpoint_callback],\n",
    "            logger=tensorboard_logger\n",
    "        )\n",
    "    return trainer\n",
    "\n",
    "def setup_model(model_dir):\n",
    "    model = FlanT5FineTuner(CONFIG[\"model_name\"], model_dir)\n",
    "    model = model.to(device)  # Explicitly move your model to the correct device\n",
    "    return model\n",
    "\n",
    "def setup_dataloaders(model, tokenizer):\n",
    "    data_path = CONFIG[\"data_dir\"] / 'transformed'\n",
    "    batch_size = CONFIG[\"batch_size\"]\n",
    "    num_workers = CONFIG[\"num_workers\"]\n",
    "\n",
    "    dataloaders = create_dataloaders(data_path, tokenizer, batch_size, num_workers)\n",
    "    return dataloaders\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Directory for models and logs\n",
    "model_timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H\")\n",
    "model_dir = Path(CONFIG[\"models_dir\"]) / f\"model_{model_timestamp}\"\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Setup Model, Trainer, and Dataloaders\n",
    "tokenizer = T5Tokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "model = setup_model(model_dir)\n",
    "dataloaders = setup_dataloaders(model, tokenizer)\n",
    "trainer = setup_trainer(model_dir)\n",
    "\n",
    "# Training\n",
    "try:\n",
    "    trainer.fit(model, dataloaders['train_supervised_small_sample'], dataloaders['dev_data_sample'])\n",
    "    trainer.test(model, dataloaders['test_data_sample'])\n",
    "except Exception as e:\n",
    "    logger.exception(\"An error occurred during training or testing.\")\n",
    "\n",
    "# Generate text and extract attentions\n",
    "input_batch = next(iter(dataloaders['dev_data_sample']))\n",
    "input_ids = input_batch['input_ids'].to(device)\n",
    "attention_mask = input_batch['attention_mask'].to(device)\n",
    "\n",
    "generated_texts, attentions = model.generate_text(input_ids, attention_mask)\n",
    "\n",
    "# Visualize attention\n",
    "model.visualize_attention_bertviz(input_ids, attention_mask, input_batch['original_ending'], attentions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c661e5957d856bb83db7ea42c976ce980a0fafd4065ab3c3a672e7d1ad452dd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
